I"ù<p><span class="highlight"> Abstract </span>
<br />
Despite the growing demand for professional graphic design knowledge, the tacit nature of design inhibits knowledge sharing. However, there is a limited understanding on the characteristics and instances of tacit knowledge in graphic design. In this work, we build a comprehensive set of tacit knowledge characteristics through a literature review. Through interviews with 10 professional graphic designers, we collected 123 tacit knowledge instances and labeled their characteristics. By qualitatively coding the instances, we identified the prominent elements, actions, and purposes of tacit knowledge. To identify which instances have been addressed the least, we conducted a systematic literature review of prior system support to graphic design. By understanding the reasons for the lack of support on these instances based on their characteristics, we propose design guidelines for capturing and applying tacit knowledge in design tools. This work takes a step towards understanding tacit knowledge, and how this knowledge can be communicated.
<br />
<br /></p>

<p class="sys-img"><img src="/assets/img/research_procedure.png" alt="Research overview. The diagram illustrates the overall research process of this work. Each research step is depicted, with specific sections marked corresponding to the description of each stage." />
<br />
The diagram shows the overall research procedures with arrows. First, <span class="highlight">(a)</span> is a literature review process for collecting tacit knowledge characteristics. <span class="highlight">(b)</span> shows that the authors did an interview study with graphic design experts to collect the tacit knowledge instances. In the study, the designers also conducted an annotation task between the instances they mentioned and the characteristics set. <span class="highlight">(c)</span> is the open coding process to analyze the instance of the collected tacit knowledge. Then, the authors conducted a systematic literature review to investigate what type of instance is still uncovered at <span class="highlight">(d)</span>. Based on these series of research steps, this paper suggests the design guidelines for capturing and applying tacit knowledge at (e).</p>

<hr />

<h2 id="interface"><span class="sys-name">Interface</span></h2>

<p class="sys-img"><img src="/assets/img/interface.png" alt="Interface of GenQuery. GenQuery shows the image search results as a gallery form. (a) Text prompt input box for text-based search: User can input a text description for the desired image here; (b) Clickable image for image-based search: An image in the gallery is clickable to provoke the image-based search. When the image is clicked, GenQuery shows similar images to the clicked one at the bottom of the gallery; (c) Like button: The user can click the like button to save the design into the side panel; (d) Generation button: To edit one of the searched images for generatin a new input, the user can click the marble emoji left top of image card. When the user clicks this button, the generation panel pops out below; (e) Show more button: This button is clicked when the user wants to see more search results." /></p>

<p><span class="sys-name">ðŸ”® GenQuery</span> provides a similar interface to popular visual search tools like <a href="https://co.pinterest.com/">Pinterest</a>. The system provides a similar interface to popular visual search tools like Pinterest. Users can input a text query <span class="highlight">[a]</span> (text-based search) or click an image in the search results <span class="highlight">[b]</span> (image-based search) to find the visual images they want to see and save the desired images <span class="highlight">[c]</span>.</p>

<p>Beyond these basic features, <span class="sys-name">GenQuery</span> supports <strong><a href="#QC" target="_self">Query Concretization</a></strong> when the user writes a query in <span class="highlight">[a]</span>. Furthermore, when the user clicks <span class="highlight">[d]</span>, it also supports <strong><a href="#IM" target="_self">Image-based Image Modification</a></strong> and <strong><a href="#KM" target="_self">Keyword-based Image Modification</a></strong> to allow the user to find more intent-aligned or diversified images.</p>

<p><br /></p>

<h3 id="query-concretization-for-text-based-search"><span id="QC" class="sys-name">Query Concretization</span> for Text-based Search</h3>

<p>Our formative study findings reveal that the visual search process is inefficient due to the userâ€™s vague text query at the initial text-based search. To address this, we propose a <span class="sys-name">Query Concretization</span> interaction using LLM prompting in the visual search process.</p>

<div class="video-wrapper">
  <iframe src="https://www.youtube.com/embed/8AhXwrU3WS4?si=tdU7Q55_YXL3ChDY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><br /></p>

<h3 id="image-based-image-modification-for-image-based-search"><span id="IM" class="sys-name">Image-based Image Modification</span> for Image-based Search</h3>

<p>When users had concrete target images they wanted to look for in their minds, they wanted to use image modality to edit the following search queries in the image-based search. To support the user in finding a more intent-aligned search result, we propose <span class="sys-name">Image-based Image Modification</span> for the following visual search query.</p>

<div class="video-wrapper">
  <iframe src="https://www.youtube.com/embed/N-F3DsbE1fI?si=-7gaKdnObBQ1g-jH" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<p><br /></p>

<h3 id="keyword-based-image-modification-for-image-based-search"><span id="KM" class="sys-name">Keyword-based Image Modification</span> for Image-based Search</h3>

<p>One of the essential aspects of visual search is finding diversified and unexpected ideas as well as searching intent-aligned images. Our formative findings identified that the users tried to use text modality when they wanted to see more diversified and different images. Thus, we propose <span class="sys-name">Text-based Image Modification</span> interaction to help the divergent visual search phase.</p>

<div class="video-wrapper">
  <iframe src="https://www.youtube.com/embed/zJTqnCh8d2w?si=-vIMSjUHxgH-o6I_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
</div>

<hr />

<p>The three interactions of <span class="sys-name">ðŸ”® GenQuery</span> allowed the user to express their intent intuitively and accurately so that the users find more satisfied, diversified, and creative ideas. If you want to see more details of the findings of our user study, please check our paper!</p>

<h2 id="bibtex">Bibtex</h2>

<pre>
@inproceedings{son2023genquery,
      title={GenQuery: Supporting Expressive Visual Search with Generative Models}, 
      author={Kihoon Son and DaEun Choi and Tae Soo Kim and Young-Ho Kim and Juho Kim},
      year={2023},
      eprint={2310.01287},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}
</pre>

<hr />

<p class="logos"><a href="https://kixlab.org"><img src="/assets/img/kixlab_logo.png" alt="Logo of KIXLAB" /></a>
<a href="https://kaist.ac.kr"><img src="/assets/img/kaist_logo.png" alt="Logo of KAIST" /></a>
<a href="https://www.facebook.com/NAVERAILAB"><img src="/assets/img/naver_logo.png" alt="Logo of NAVER" /></a></p>

<p class="center acknowledgement">This research was supported by the <strong>KAIST-NAVER Hypercreative AI Center</strong>.</p>
:ET